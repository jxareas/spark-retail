{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "898e1e15bf9a05a3",
   "metadata": {},
   "source": [
    "# Procesamiento y Análisis de Datos con Apache Spark\n",
    "\n",
    "En este proyecto, se utiliza Apache Spark para resolver una serie de tareas de análisis de datos relacionadas con una empresa global del sector retail, que tiene tanto tiendas físicas como ventas online.\n",
    "\n",
    "El análisis se implementa utilizando [Apache Spark][Apache Spark], un motor de procesamiento de datos distribuido y de alto rendimiento, particularmente mediante [PySpark][PySpark], la API de Apache Spark para el entorno de Python.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../assets/logos/tools_apache_spark.svg\" height=\"225\" width=\"225\"/>\n",
    "</div>\n",
    "\n",
    "[Apache Spark]: https://spark.apache.org\n",
    "[PySpark]: https://spark.apache.org/docs/latest/api/python/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37e4e5cd61bf647",
   "metadata": {},
   "source": [
    "## Descripción de la tarea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8715f88f54b8a2b8",
   "metadata": {},
   "source": [
    "Habéis sido contratados por una empresa perteneciente al sector del Retail.\n",
    "\n",
    "Es una empresa con presencia a nivel mundial con sede en España. Tiene tanto tiendas físicas, como venta on-line.\n",
    "\n",
    "Todos los días recibe un archivo llamado purchases.json con compras realizadas en todo el mundo.\n",
    "\n",
    "Cada línea del fichero es una compra de una unidad del producto correspondiente.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../assets/images/purchases.png\" height=\"600\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "La plataforma logística envía todos los días un archivo stock.csv con el stock de cada producto:\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../assets/images/stocks.png\" height=\"600\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "**IMPORTANTE**\n",
    "\n",
    "Los datos se han generado de forma aleatoria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1355faecdb1929",
   "metadata": {},
   "source": [
    "## Cargando Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986a94d517cb903c",
   "metadata": {},
   "source": [
    "Primeramente, importamos las librerías necesarias para ejecutar nuestro código, principalmente aquellas relacionadas con la API de PySpark."
   ]
  },
  {
   "cell_type": "code",
   "id": "97c926500c44780d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T10:00:46.386993Z",
     "start_time": "2025-04-28T10:00:46.383929Z"
    }
   },
   "source": [
    "# Importing libraries\n",
    "\n",
    "from pyspark.sql import SparkSession, Window\n",
    "import pyspark.sql.functions as F"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "id": "de41c7d2c3e0e2e",
   "metadata": {},
   "source": [
    "Luego, inicializamos una sesión de Spark utilizando la API de PySpark. La función `SparkSession.builder` se usa para configurar y crear una nueva instancia de Spark.\n",
    "\n",
    "El parámetro `appName` establece el nombre de la aplicación, que será útil para identificar la sesión de Spark en el monitor de Spark. Finalmente, el método `getOrCreate` se asegura de que se obtenga una sesión existente si ya está en ejecución, o se cree una nueva si no existe.\n",
    "\n",
    "Esta sesión de Spark será el punto de entrada para trabajar con los datos y ejecutar las operaciones de análisis de datos."
   ]
  },
  {
   "cell_type": "code",
   "id": "795ab26f8227e7f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T10:00:46.417400Z",
     "start_time": "2025-04-28T10:00:46.409569Z"
    }
   },
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"RetailAnalytics\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7a0323d20bb0>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.68.65:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>RetailAnalytics</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "id": "4e53029ae638a006",
   "metadata": {},
   "source": [
    "## Cargando la data\n",
    "\n",
    "Procedemos a cargar los archivos que contienen los datos sobre compras (`purchases.json`) e inventario (`stock.csv`)."
   ]
  },
  {
   "cell_type": "code",
   "id": "fa3d612bb45e8bd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T10:00:46.743906Z",
     "start_time": "2025-04-28T10:00:46.503294Z"
    }
   },
   "source": [
    "# Setting parameters, data file paths\n",
    "PURCHASES_DATA_PATH = \"../data/purchases.json\"\n",
    "STOCK_DATA_PATH = \"../data/stock.csv\"\n",
    "\n",
    "# Loading the purchases json into a dataframe\n",
    "purchases_df = spark.read.json(PURCHASES_DATA_PATH)\n",
    "# Loading the stocks csv into a dataframe\n",
    "stock_df = spark.read.csv(STOCK_DATA_PATH, header=True, inferSchema=True)"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Procedemos a mostrar los cinco primeros datos de nuestro dataframe de compras:",
   "id": "f3f728ab31f3fc48"
  },
  {
   "cell_type": "code",
   "id": "71e906892b024a28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T10:00:46.885291Z",
     "start_time": "2025-04-28T10:00:46.825743Z"
    }
   },
   "source": "purchases_df.show(n=5)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+------------+-------+----------+-------+-----+\n",
      "|item_type|            location|payment_type|  price|product_id|shop_id|  way|\n",
      "+---------+--------------------+------------+-------+----------+-------+-----+\n",
      "|     shoe|   {7.0814, 56.5147}|      paypal|58.6024|        12|     36|store|\n",
      "|    shirt| {20.7594, 169.8936}|        card|29.6226|        14|     15|store|\n",
      "|     shoe| {93.5029, 159.9378}|        cash|33.0585|        11|     37|store|\n",
      "|     shoe|{59.1923, -178.1721}|      paypal|97.9282|        81|     90|store|\n",
      "|     jean| {62.8714, 128.6758}|      paypal| 1.9822|        14|     61|  web|\n",
      "+---------+--------------------+------------+-------+----------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Asimismo, mostramos el esquema del dataframe de compras:",
   "id": "e59940f106672a15"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T10:00:47.023652Z",
     "start_time": "2025-04-28T10:00:47.019645Z"
    }
   },
   "cell_type": "code",
   "source": "purchases_df.printSchema()",
   "id": "2d083fee6125a8d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- item_type: string (nullable = true)\n",
      " |-- location: struct (nullable = true)\n",
      " |    |-- lat: double (nullable = true)\n",
      " |    |-- lon: double (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- shop_id: long (nullable = true)\n",
      " |-- way: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Procedemos a mostrar los cinco primeros datos de nuestro dataframe de inventario:",
   "id": "b8c6cc18c720240b"
  },
  {
   "cell_type": "code",
   "id": "c3b1c6ef8d45fd7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T10:00:47.167061Z",
     "start_time": "2025-04-28T10:00:47.127422Z"
    }
   },
   "source": "stock_df.show(n=5)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|product_id|quantity|\n",
      "+----------+--------+\n",
      "|         1|      34|\n",
      "|         2|      97|\n",
      "|         3|      70|\n",
      "|         4|      66|\n",
      "|         5|      75|\n",
      "+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Asimismo, mostramos el esquema del dataframe de inventario:",
   "id": "d45db0f3a41deb92"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T10:00:47.290655Z",
     "start_time": "2025-04-28T10:00:47.287377Z"
    }
   },
   "cell_type": "code",
   "source": "stock_df.printSchema()",
   "id": "88ee4ff5c9d89c73",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "id": "68aa96d447b00a03",
   "metadata": {},
   "source": [
    "## Solución de la tarea\n",
    "\n",
    "Debéis crear un programa Spark 2.x utilizando el lenguaje Python y resolver las siguientes tareas (usando tanto del DataFrame API como Spark SQL):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b3b91ce1770a32",
   "metadata": {},
   "source": [
    "### 1. Los 10 productos más comprados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4346bbfbaa4662ce",
   "metadata": {},
   "source": [
    "#### Spark DataFrame:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T10:00:47.489296Z",
     "start_time": "2025-04-28T10:00:47.384063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "top_10_products_df = (\n",
    "    purchases_df.groupBy(\"product_id\")\n",
    "    .agg(F.count(\"*\").alias(\"total_purchases\"))\n",
    "    .orderBy(\"total_purchases\", ascending=False)\n",
    "    .limit(10)\n",
    ")\n",
    "\n",
    "top_10_products_df.show()"
   ],
   "id": "b6489b134f25d05c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n",
      "|product_id|total_purchases|\n",
      "+----------+---------------+\n",
      "|        64|             50|\n",
      "|        81|             45|\n",
      "|         3|             44|\n",
      "|        61|             43|\n",
      "|        31|             43|\n",
      "|        36|             43|\n",
      "|        60|             43|\n",
      "|        69|             42|\n",
      "|        30|             42|\n",
      "|         7|             41|\n",
      "+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "id": "a40f0e125c0d64ef",
   "metadata": {},
   "source": [
    "#### SQL:\n",
    "\n",
    "Primeramente, debemos crear unas vistas temporales:"
   ]
  },
  {
   "cell_type": "code",
   "id": "3480022089621bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T10:00:47.619474Z",
     "start_time": "2025-04-28T10:00:47.608476Z"
    }
   },
   "source": [
    "purchases_df.createOrReplaceTempView(\"purchases\")\n",
    "stock_df.createOrReplaceTempView(\"stock\")"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "id": "b9e770b265da648c",
   "metadata": {},
   "source": [
    "Una vez hecho esto, ejecutamos la consulta en SQL nativo:"
   ]
  },
  {
   "cell_type": "code",
   "id": "5e22f457f3dccc55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T10:00:47.818612Z",
     "start_time": "2025-04-28T10:00:47.699375Z"
    }
   },
   "source": [
    "top_10_products_sql = spark.sql(\"\"\"\n",
    "                                SELECT product_id, COUNT(*) as total_purchases\n",
    "                                FROM purchases\n",
    "                                GROUP BY product_id\n",
    "                                ORDER BY total_purchases DESC LIMIT 10\n",
    "                                \"\"\")\n",
    "\n",
    "top_10_products_sql.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n",
      "|product_id|total_purchases|\n",
      "+----------+---------------+\n",
      "|        64|             50|\n",
      "|        81|             45|\n",
      "|         3|             44|\n",
      "|        61|             43|\n",
      "|        31|             43|\n",
      "|        36|             43|\n",
      "|        60|             43|\n",
      "|        69|             42|\n",
      "|        30|             42|\n",
      "|         7|             41|\n",
      "+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "id": "9fd9ebbdf85035f6",
   "metadata": {},
   "source": [
    "### 2. Porcentaje de compra de cada tipo de producto (`item_type`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73676a2094a5e56",
   "metadata": {},
   "source": [
    "#### Spark DataFrame:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T10:00:48.258478Z",
     "start_time": "2025-04-28T10:00:47.981607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "item_type_percentage_df = (\n",
    "    purchases_df.groupBy(\"item_type\")\n",
    "    .agg((F.count(\"*\") / purchases_df.count() * 100).alias(\"purchase_percentage\"))\n",
    "    .orderBy(\"purchase_percentage\", ascending=False)\n",
    "    .withColumn(\"purchase_percentage\", F.round(F.col(\"purchase_percentage\"), 2))\n",
    ")\n",
    "\n",
    "item_type_percentage_df.show()"
   ],
   "id": "667902c3cc0a5019",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+\n",
      "|item_type|purchase_percentage|\n",
      "+---------+-------------------+\n",
      "|     shoe|              20.88|\n",
      "|     jean|              20.76|\n",
      "|    shirt|              19.58|\n",
      "|  trouser|              19.55|\n",
      "|   jacket|              19.22|\n",
      "+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "id": "22aa11345c39e01",
   "metadata": {},
   "source": [
    "#### SQL:"
   ]
  },
  {
   "cell_type": "code",
   "id": "b4bfb9a657409d76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T10:00:48.742916Z",
     "start_time": "2025-04-28T10:00:48.445734Z"
    }
   },
   "source": [
    "item_type_percentage_sql = spark.sql(\"\"\"\n",
    "                                     SELECT item_type,\n",
    "                                            ROUND((COUNT(*) * 100.0 / (SELECT COUNT(*) FROM purchases)), 2) AS percentage\n",
    "                                     FROM purchases\n",
    "                                     GROUP BY item_type\n",
    "                                     ORDER BY percentage DESC\n",
    "                                     \"\"\")\n",
    "\n",
    "item_type_percentage_sql.show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|item_type|percentage|\n",
      "+---------+----------+\n",
      "|     shoe|     20.88|\n",
      "|     jean|     20.76|\n",
      "|    shirt|     19.58|\n",
      "|  trouser|     19.55|\n",
      "|   jacket|     19.22|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "id": "4c1ac58457acc8",
   "metadata": {},
   "source": [
    "### 3. Obtener los 3 productos más comprados por cada tipo de producto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef5631ba6aec8cb",
   "metadata": {},
   "source": [
    "#### Spark DataFrame:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T10:00:49.252779Z",
     "start_time": "2025-04-28T10:00:48.813075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "window_spec = Window.partitionBy(\"item_type\").orderBy(F.desc(\"purchase_count\"))\n",
    "\n",
    "top_3_most_purchased_products_df = (\n",
    "    purchases_df.groupBy(\"product_id\", \"item_type\")\n",
    "    .agg(F.count(\"*\").alias(\"purchase_count\"))\n",
    "    .withColumn(\"rank\", F.row_number().over(window_spec))\n",
    "    .filter(F.col(\"rank\") <= 3)\n",
    ")\n",
    "\n",
    "top_3_most_purchased_products_df.show()"
   ],
   "id": "62dadd84eb543d8e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------+----+\n",
      "|product_id|item_type|purchase_count|rank|\n",
      "+----------+---------+--------------+----+\n",
      "|        12|   jacket|            12|   1|\n",
      "|        64|   jacket|            12|   2|\n",
      "|        45|   jacket|            11|   3|\n",
      "|        76|     jean|            14|   1|\n",
      "|        90|     jean|            12|   2|\n",
      "|        47|     jean|            11|   3|\n",
      "|         3|    shirt|            13|   1|\n",
      "|        54|    shirt|            12|   2|\n",
      "|        59|    shirt|            12|   3|\n",
      "|        85|     shoe|            16|   1|\n",
      "|        69|     shoe|            15|   2|\n",
      "|        96|     shoe|            15|   3|\n",
      "|        23|  trouser|            13|   1|\n",
      "|        25|  trouser|            12|   2|\n",
      "|        37|  trouser|            12|   3|\n",
      "+----------+---------+--------------+----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "id": "9b677ac5b39c17cc",
   "metadata": {},
   "source": [
    "#### Spark SQL Nativo:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T10:00:49.610063Z",
     "start_time": "2025-04-28T10:00:49.328431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "top_3_most_purchased_products_sql = spark.sql(\"\"\"\n",
    "                                              WITH ranked_products AS (SELECT product_id,\n",
    "                                                                              item_type,\n",
    "                                                                              COUNT(*)                                                          AS purchase_count,\n",
    "                                                                              ROW_NUMBER() OVER (PARTITION BY item_type ORDER BY COUNT(*) DESC) AS rank\n",
    "                                                                       FROM purchases\n",
    "                                                                       GROUP BY product_id, item_type)\n",
    "                                              SELECT *\n",
    "                                              FROM ranked_products\n",
    "                                              WHERE rank <= 3\n",
    "                                              \"\"\")\n",
    "\n",
    "top_3_most_purchased_products_sql.show()"
   ],
   "id": "11440b44fae20b01",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------+----+\n",
      "|product_id|item_type|purchase_count|rank|\n",
      "+----------+---------+--------------+----+\n",
      "|        12|   jacket|            12|   1|\n",
      "|        64|   jacket|            12|   2|\n",
      "|        45|   jacket|            11|   3|\n",
      "|        76|     jean|            14|   1|\n",
      "|        90|     jean|            12|   2|\n",
      "|        47|     jean|            11|   3|\n",
      "|         3|    shirt|            13|   1|\n",
      "|        54|    shirt|            12|   2|\n",
      "|        59|    shirt|            12|   3|\n",
      "|        85|     shoe|            16|   1|\n",
      "|        69|     shoe|            15|   2|\n",
      "|        96|     shoe|            15|   3|\n",
      "|        23|  trouser|            13|   1|\n",
      "|        25|  trouser|            12|   2|\n",
      "|        37|  trouser|            12|   3|\n",
      "+----------+---------+--------------+----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "id": "8354d495e516709d",
   "metadata": {},
   "source": [
    "### 4. Obtener los productos que son más caros que la media del precio de los productos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e04db0d1558fb9",
   "metadata": {},
   "source": [
    "#### Spark DataFrame:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T10:00:50.107211Z",
     "start_time": "2025-04-28T10:00:49.748784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mean_price = purchases_df.agg(F.mean('price')).collect()[0][0]\n",
    "\n",
    "products_above_avg_df = (\n",
    "    purchases_df\n",
    "    .select('product_id', 'item_type', 'price')\n",
    "    .filter(F.col('price') > mean_price)\n",
    "    .distinct()\n",
    "    .orderBy(F.asc('price'))\n",
    "    .withColumn('average_price', F.lit(round(mean_price, 2)))\n",
    ")\n",
    "\n",
    "products_above_avg_df.show()"
   ],
   "id": "203e0599088cd967",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+-------------+\n",
      "|product_id|item_type|  price|average_price|\n",
      "+----------+---------+-------+-------------+\n",
      "|         1|     shoe|49.7916|        49.78|\n",
      "|        94|  trouser|49.7948|        49.78|\n",
      "|        10|     jean|49.8007|        49.78|\n",
      "|        86|  trouser|49.8073|        49.78|\n",
      "|        54|  trouser|49.8356|        49.78|\n",
      "|        94|  trouser|49.8365|        49.78|\n",
      "|        40|     shoe|49.8797|        49.78|\n",
      "|        51|   jacket|49.8858|        49.78|\n",
      "|        52|    shirt|49.8867|        49.78|\n",
      "|        60|  trouser|49.9317|        49.78|\n",
      "|         6|    shirt|49.9319|        49.78|\n",
      "|        92|   jacket|49.9374|        49.78|\n",
      "|        30|     shoe| 49.963|        49.78|\n",
      "|        85|   jacket|49.9758|        49.78|\n",
      "|        15|    shirt|50.0076|        49.78|\n",
      "|        71|    shirt|50.0145|        49.78|\n",
      "|         8|    shirt|50.0415|        49.78|\n",
      "|        38|  trouser|  50.06|        49.78|\n",
      "|        46|   jacket|50.0828|        49.78|\n",
      "|        90|     jean|50.1206|        49.78|\n",
      "+----------+---------+-------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "id": "61f138c9fc5de53c",
   "metadata": {},
   "source": [
    "#### Spark SQL Nativo:"
   ]
  },
  {
   "cell_type": "code",
   "id": "ca8126be5b68f294",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T10:00:50.591277Z",
     "start_time": "2025-04-28T10:00:50.265489Z"
    }
   },
   "source": [
    "products_above_avg_sql = spark.sql(\"\"\"\n",
    "                                   WITH average_price AS (SELECT AVG(price) AS avg_price\n",
    "                                                          FROM purchases)\n",
    "                                   SELECT DISTINCT p.product_id,\n",
    "                                                   p.item_type,\n",
    "                                                   p.price,\n",
    "                                                   ROUND(a.avg_price, 2) AS average_price\n",
    "                                   FROM purchases p\n",
    "                                            CROSS JOIN\n",
    "                                        average_price a\n",
    "                                   WHERE p.price > a.avg_price\n",
    "                                   ORDER BY p.price ASC\n",
    "                                   \"\"\")\n",
    "\n",
    "products_above_avg_sql.count()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1681"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "id": "e381acac2bf589fb",
   "metadata": {},
   "source": [
    "\n",
    "### 5. Indicar la tienda que ha vendido más productos."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Spark DataFrame:",
   "id": "a3737acb9b2e6dd7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T10:00:50.810937Z",
     "start_time": "2025-04-28T10:00:50.658751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "top_store_by_sales_df = (\n",
    "    purchases_df\n",
    "    .groupBy('shop_id')\n",
    "    .agg(F.count('*').alias('total_sales'))\n",
    "    .orderBy(F.desc('total_sales'))\n",
    "    .limit(1)\n",
    ")\n",
    "\n",
    "top_store_by_sales_df.show()"
   ],
   "id": "e84fe9ccf6a8bea7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|shop_id|total_sales|\n",
      "+-------+-----------+\n",
      "|     69|         47|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Spark SQL Nativo:",
   "id": "bafbecf2e9ae82b7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T10:00:51.058139Z",
     "start_time": "2025-04-28T10:00:50.945057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "top_store_by_sales_sql = spark.sql(\"\"\"\n",
    "                                   SELECT shop_id,\n",
    "                                          COUNT(*) AS total_sales\n",
    "                                   FROM purchases\n",
    "                                   GROUP BY shop_id\n",
    "                                   ORDER BY total_sales DESC LIMIT 1\n",
    "                                   \"\"\")\n",
    "\n",
    "top_store_by_sales_sql.show()\n"
   ],
   "id": "333ab5cf1e714ca0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|shop_id|total_sales|\n",
      "+-------+-----------+\n",
      "|     69|         47|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "id": "dab631f498019928",
   "metadata": {},
   "source": [
    "### 6. Indicar la tienda que ha facturado más dinero."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Spark DataFrame:",
   "id": "194c2c1e40f477c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T10:00:51.316407Z",
     "start_time": "2025-04-28T10:00:51.134731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "top_store_by_revenue_df = (\n",
    "    purchases_df\n",
    "    .groupBy('shop_id')\n",
    "    .agg(F.round(F.sum('price'), 2).alias('revenue'))\n",
    "    .orderBy(F.desc('revenue'))\n",
    "    .limit(1)\n",
    ")\n",
    "\n",
    "top_store_by_revenue_df.show()"
   ],
   "id": "fd8409bda92c530f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|shop_id|revenue|\n",
      "+-------+-------+\n",
      "|     69|2444.89|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Spark SQL Nativo:",
   "id": "5958a80ee87aa62b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T10:00:51.596809Z",
     "start_time": "2025-04-28T10:00:51.479642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "top_store_by_revenue_sql = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    shop_id,\n",
    "    ROUND(SUM(price), 2) AS revenue\n",
    "FROM purchases\n",
    "GROUP BY shop_id\n",
    "ORDER BY revenue DESC\n",
    "LIMIT 1\n",
    "\"\"\")\n",
    "\n",
    "top_store_by_revenue_sql.show()"
   ],
   "id": "97ab6b786a5c88d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|shop_id|revenue|\n",
      "+-------+-------+\n",
      "|     69|2444.89|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "cell_type": "markdown",
   "id": "e907912aa69217d6",
   "metadata": {},
   "source": [
    "### 7. Dividir el mundo en 5 áreas geográficas iguales según la longitud (location.lon) y agregar una columna con el nombre del área geográfica (Area1: - 180 a - 108, Area2: - 108 a - 36, Area3: - 36 a 36, Area4: 36 a 108, Area5: 108 a 180), ..."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Spark DataFrame:",
   "id": "a762c31fc5776885"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T10:00:51.669763Z",
     "start_time": "2025-04-28T10:00:51.665259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def assign_area(longitude: int) -> str:\n",
    "    \"\"\"\n",
    "    Assigns a geographic area label based on the given longitude value.\n",
    "\n",
    "    Longitude ranges are divided into the following areas:\n",
    "        - Area1: [-180, -108)\n",
    "        - Area2: [-108, -36)\n",
    "        - Area3: [-36, 36)\n",
    "        - Area4: [36, 108)\n",
    "        - Area5: [108, 180]\n",
    "        - Area6: For values outside the standard longitude range\n",
    "\n",
    "    Args:\n",
    "        longitude (int): The longitude value to classify.\n",
    "\n",
    "    Returns:\n",
    "        str: The name of the area corresponding to the given longitude.\n",
    "    \"\"\"\n",
    "    if -180 <= longitude < -108: return \"Area1\"\n",
    "    elif -108 <= longitude < -36: return \"Area2\"\n",
    "    elif -36 <= longitude < 36: return \"Area3\"\n",
    "    elif 36 <= longitude < 108: return \"Area4\"\n",
    "    elif 108 <= longitude <= 180: return \"Area5\"\n",
    "    else: return \"Area6\""
   ],
   "id": "619e98910ce487ba",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T10:00:51.801172Z",
     "start_time": "2025-04-28T10:00:51.780106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "area_udf = F.udf(assign_area)\n",
    "\n",
    "purchases_with_area = purchases_df.withColumn(\n",
    "    \"area\",\n",
    "    area_udf(F.col(\"location.lon\"))\n",
    ")"
   ],
   "id": "77fdbb12be64daf5",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Spark SQL Nativo:",
   "id": "b99ea71c75670e68"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T10:00:51.890505Z",
     "start_time": "2025-04-28T10:00:51.871642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT *,\n",
    "  CASE\n",
    "    WHEN location.lon >= -180 AND location.lon < -108 THEN 'Area1'\n",
    "    WHEN location.lon >= -108 AND location.lon < -36 THEN 'Area2'\n",
    "    WHEN location.lon >= -36 AND location.lon < 36 THEN 'Area3'\n",
    "    WHEN location.lon >= 36 AND location.lon < 108 THEN 'Area4'\n",
    "    WHEN location.lon >= 108 AND location.lon <= 180 THEN 'Area5'\n",
    "    ELSE 'Area6'\n",
    "  END AS area\n",
    "FROM purchases\n",
    "\"\"\").createOrReplaceTempView(\"purchases_with_area\")"
   ],
   "id": "bf2d2d98849a9c96",
   "outputs": [],
   "execution_count": 54
  },
  {
   "cell_type": "markdown",
   "id": "b9bcfb6c367184d",
   "metadata": {},
   "source": [
    "#### 7.1. ¿En qué área se utiliza más PayPal?"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Spark DataFrame:",
   "id": "eb1097da702fdde"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T10:00:52.169030Z",
     "start_time": "2025-04-28T10:00:51.965023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "top_paypal_by_area_df = (\n",
    "    purchases_with_area\n",
    "    .filter(F.col(\"payment_type\") == \"paypal\")\n",
    "    .groupBy(\"area\")\n",
    "    .count()\n",
    "    .orderBy(F.desc(\"count\"))\n",
    "    .limit(1)\n",
    ")\n",
    "\n",
    "top_paypal_by_area_df.show()"
   ],
   "id": "d1eac6b487c48ccb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| area|count|\n",
      "+-----+-----+\n",
      "|Area4|  241|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Spark SQL Nativo:",
   "id": "3223df8e57fc0036"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T10:00:52.435368Z",
     "start_time": "2025-04-28T10:00:52.274381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "top_paypal_by_area_sql = spark.sql(\"\"\"\n",
    "SELECT area, COUNT(*) as paypal_count\n",
    "FROM purchases_with_area\n",
    "WHERE payment_type = 'paypal'\n",
    "GROUP BY area\n",
    "ORDER BY paypal_count DESC\n",
    "LIMIT 1\n",
    "\"\"\")\n",
    "\n",
    "top_paypal_by_area_sql.show()"
   ],
   "id": "f336e3b59236ea0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "| area|paypal_count|\n",
      "+-----+------------+\n",
      "|Area4|         241|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "cell_type": "markdown",
   "id": "7aa74a45c156c39d",
   "metadata": {},
   "source": [
    "#### 7.2. ¿Cuáles son los 3 productos más comprados en cada área?"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Spark DataFrame:",
   "id": "61fcd620aca0a4a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T10:00:52.903754Z",
     "start_time": "2025-04-28T10:00:52.565357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "window_spec = Window.partitionBy(\"area\").orderBy(F.desc(\"count\"))\n",
    "\n",
    "top_products_by_area_df = (\n",
    "    purchases_with_area\n",
    "    .groupBy(\"area\", \"product_id\")\n",
    "    .count()\n",
    "    .withColumn(\"rank\", F.row_number().over(window_spec))\n",
    "    .filter(F.col(\"rank\") <= 3)\n",
    "    .orderBy(\"area\", \"rank\")\n",
    ")\n",
    "\n",
    "top_products_by_area_df.show()"
   ],
   "id": "68163c06b78a8b1f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+----+\n",
      "| area|product_id|count|rank|\n",
      "+-----+----------+-----+----+\n",
      "|Area1|        66|   18|   1|\n",
      "|Area1|        37|   13|   2|\n",
      "|Area1|         3|   13|   3|\n",
      "|Area2|        25|   12|   1|\n",
      "|Area2|        81|   11|   2|\n",
      "|Area2|        11|   11|   3|\n",
      "|Area3|        54|   14|   1|\n",
      "|Area3|        76|   13|   2|\n",
      "|Area3|        61|   13|   3|\n",
      "|Area4|         7|   13|   1|\n",
      "|Area4|        47|   13|   2|\n",
      "|Area4|        28|   12|   3|\n",
      "|Area5|        60|   18|   1|\n",
      "|Area5|        64|   12|   2|\n",
      "|Area5|        30|   12|   3|\n",
      "+-----+----------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Spark SQL Nativo:",
   "id": "7a3c9688c3fc6bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T10:00:53.268700Z",
     "start_time": "2025-04-28T10:00:53.044429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "top_products_by_area_sql = spark.sql(\"\"\"\n",
    "WITH ranked_products_area AS (\n",
    "    SELECT\n",
    "        area,\n",
    "        product_id,\n",
    "        COUNT(*) AS count,\n",
    "        ROW_NUMBER() OVER (PARTITION BY area ORDER BY COUNT(*) DESC) AS rank\n",
    "    FROM purchases_with_area\n",
    "    GROUP BY area, product_id\n",
    ")\n",
    "SELECT *\n",
    "FROM ranked_products_area\n",
    "WHERE rank <= 3\n",
    "\"\"\")\n",
    "\n",
    "top_products_by_area_sql.show()"
   ],
   "id": "1103b57b8ca8e2e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+----+\n",
      "| area|product_id|count|rank|\n",
      "+-----+----------+-----+----+\n",
      "|Area1|        66|   18|   1|\n",
      "|Area1|        37|   13|   2|\n",
      "|Area1|         3|   13|   3|\n",
      "|Area2|        25|   12|   1|\n",
      "|Area2|        81|   11|   2|\n",
      "|Area2|        11|   11|   3|\n",
      "|Area3|        54|   14|   1|\n",
      "|Area3|        76|   13|   2|\n",
      "|Area3|        61|   13|   3|\n",
      "|Area4|         7|   13|   1|\n",
      "|Area4|        47|   13|   2|\n",
      "|Area4|        28|   12|   3|\n",
      "|Area5|        60|   18|   1|\n",
      "|Area5|        64|   12|   2|\n",
      "|Area5|        30|   12|   3|\n",
      "+-----+----------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "cell_type": "markdown",
   "id": "73f35986ad256e0d",
   "metadata": {},
   "source": [
    "#### 7.3. ¿Qué área ha facturado menos dinero?"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Spark DataFrame:",
   "id": "3c2b7543607c720"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T10:00:53.608007Z",
     "start_time": "2025-04-28T10:00:53.344303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lowest_revenue_area_df = (\n",
    "    purchases_with_area\n",
    "    .groupBy(\"area\")\n",
    "    .agg(F.round(F.sum(\"price\"), 2).alias(\"revenue\"))\n",
    "    .orderBy(\"revenue\")\n",
    "    .limit(1)\n",
    ")\n",
    "\n",
    "lowest_revenue_area_df.show()"
   ],
   "id": "fb851e435d926ee5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "| area| revenue|\n",
      "+-----+--------+\n",
      "|Area1|32213.25|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Spark SQL Nativo:",
   "id": "6e5262bc880e50a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T10:00:53.906572Z",
     "start_time": "2025-04-28T10:00:53.733781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lowest_revenue_area_sql = spark.sql(\"\"\"\n",
    "SELECT area, ROUND(SUM(price), 2) as revenue\n",
    "FROM purchases_with_area\n",
    "GROUP BY area\n",
    "ORDER BY revenue ASC\n",
    "LIMIT 1\n",
    "\"\"\")\n",
    "\n",
    "lowest_revenue_area_sql.show()"
   ],
   "id": "f08ee6abdb430286",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "| area| revenue|\n",
      "+-----+--------+\n",
      "|Area1|32213.25|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "cell_type": "markdown",
   "id": "1ff3c4e2833b7ef",
   "metadata": {},
   "source": [
    "### 8. Indicar los productos que no tienen stock suficiente para las compras realizadas."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Spark DataFrame:",
   "id": "189aeea593caea3a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T10:00:54.213625Z",
     "start_time": "2025-04-28T10:00:54.045346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "purchases_count_df = (\n",
    "    purchases_df\n",
    "    .groupBy(\"product_id\")\n",
    "    .count()\n",
    "    .withColumnRenamed(\"count\", \"purchases_made\")\n",
    ")\n",
    "\n",
    "insufficient_stock_df = (\n",
    "    stock_df\n",
    "    .join(purchases_count_df, \"product_id\")\n",
    "    .filter(F.col(\"quantity\") < F.col(\"purchases_made\"))\n",
    "    .withColumnRenamed(\"quantity\", \"quantity_in_stock\")\n",
    ")\n",
    "\n",
    "insufficient_stock_df.show()"
   ],
   "id": "2e6e2d978ff14841",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+--------------+\n",
      "|product_id|quantity_in_stock|purchases_made|\n",
      "+----------+-----------------+--------------+\n",
      "|        29|               25|            38|\n",
      "|         1|               34|            39|\n",
      "|        37|               22|            38|\n",
      "+----------+-----------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Spark SQL Nativo:",
   "id": "59b0baa6475fd380"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T10:00:54.444517Z",
     "start_time": "2025-04-28T10:00:54.319841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "insufficient_stock_sql = spark.sql(\"\"\"\n",
    "    WITH purchase_counts AS (\n",
    "        SELECT\n",
    "            product_id,\n",
    "            COUNT(*) AS purchases\n",
    "        FROM purchases\n",
    "        GROUP BY product_id\n",
    "    )\n",
    "    SELECT\n",
    "        s.product_id,\n",
    "        s.quantity AS quantity_in_stock,\n",
    "        p.purchases AS purchases_made\n",
    "    FROM stock s\n",
    "    JOIN purchase_counts p ON s.product_id = p.product_id\n",
    "    WHERE s.quantity < p.purchases\n",
    "\"\"\")\n",
    "\n",
    "insufficient_stock_sql.show()"
   ],
   "id": "6c3afea3055a89a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+--------------+\n",
      "|product_id|quantity_in_stock|purchases_made|\n",
      "+----------+-----------------+--------------+\n",
      "|        29|               25|            38|\n",
      "|         1|               34|            39|\n",
      "|        37|               22|            38|\n",
      "+----------+-----------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 62
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
