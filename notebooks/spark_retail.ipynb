{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "898e1e15bf9a05a3",
   "metadata": {},
   "source": [
    "# Procesamiento y Análisis de Datos con Apache Spark\n",
    "\n",
    "En este proyecto, se utiliza Apache Spark para resolver una serie de tareas de análisis de datos relacionadas con una empresa global del sector retail, que tiene tanto tiendas físicas como ventas online.\n",
    "\n",
    "El análisis se implementa utilizando [Apache Spark][Apache Spark], un motor de procesamiento de datos distribuido y de alto rendimiento, particularmente mediante [PySpark][PySpark], la API de Apache Spark para el entorno de Python.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../assets/logos/tools_apache_spark.svg\" height=\"225\" width=\"225\"/>\n",
    "</div>\n",
    "\n",
    "[Apache Spark]: https://spark.apache.org\n",
    "[PySpark]: https://spark.apache.org/docs/latest/api/python/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37e4e5cd61bf647",
   "metadata": {},
   "source": [
    "## Descripción de la tarea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8715f88f54b8a2b8",
   "metadata": {},
   "source": [
    "Habéis sido contratados por una empresa perteneciente al sector del Retail.\n",
    "\n",
    "Es una empresa con presencia a nivel mundial con sede en España. Tiene tanto tiendas físicas, como venta on-line.\n",
    "\n",
    "Todos los días recibe un archivo llamado purchases.json con compras realizadas en todo el mundo.\n",
    "\n",
    "Cada línea del fichero es una compra de una unidad del producto correspondiente.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../assets/images/purchases.png\" height=\"600\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "La plataforma logística envía todos los días un archivo stock.csv con el stock de cada producto:\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"../assets/images/stocks.png\" height=\"600\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "**IMPORTANTE**\n",
    "\n",
    "Los datos se han generado de forma aleatoria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1355faecdb1929",
   "metadata": {},
   "source": [
    "## Cargando Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986a94d517cb903c",
   "metadata": {},
   "source": [
    "Primeramente, importamos las librerías necesarias para ejecutar nuestro código, principalmente aquellas relacionadas con la API de PySpark."
   ]
  },
  {
   "cell_type": "code",
   "id": "97c926500c44780d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T10:31:33.768624Z",
     "start_time": "2025-04-27T10:31:33.729507Z"
    }
   },
   "source": [
    "# Importing libraries\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "de41c7d2c3e0e2e",
   "metadata": {},
   "source": [
    "Luego, inicializamos una sesión de Spark utilizando la API de PySpark. La función `SparkSession.builder` se usa para configurar y crear una nueva instancia de Spark.\n",
    "\n",
    "El parámetro `appName` establece el nombre de la aplicación, que será útil para identificar la sesión de Spark en el monitor de Spark. Finalmente, el método `getOrCreate` se asegura de que se obtenga una sesión existente si ya está en ejecución, o se cree una nueva si no existe.\n",
    "\n",
    "Esta sesión de Spark será el punto de entrada para trabajar con los datos y ejecutar las operaciones de análisis de datos."
   ]
  },
  {
   "cell_type": "code",
   "id": "795ab26f8227e7f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T10:39:11.760856Z",
     "start_time": "2025-04-27T10:39:11.749582Z"
    }
   },
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"RetailAnalytics\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x70f2a55e8c10>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.68.65:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>RetailAnalytics</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "4e53029ae638a006",
   "metadata": {},
   "source": [
    "## Cargando la data"
   ]
  },
  {
   "cell_type": "code",
   "id": "fa3d612bb45e8bd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T10:31:39.261318Z",
     "start_time": "2025-04-27T10:31:36.615735Z"
    }
   },
   "source": [
    "# Setting parameters, data file paths\n",
    "PURCHASES_DATA_PATH = \"../data/purchases.json\"\n",
    "STOCKS_DATA_PATH = \"../data/stocks.csv\"\n",
    "\n",
    "# Loading the purchases json into a dataframe\n",
    "purchases_df = spark.read.json(PURCHASES_DATA_PATH)\n",
    "# Loading the stocks csv into a dataframe\n",
    "stock_df = spark.read.csv(STOCKS_DATA_PATH, header=True, inferSchema=True)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "71e906892b024a28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T10:31:39.476251Z",
     "start_time": "2025-04-27T10:31:39.280915Z"
    }
   },
   "source": [
    "purchases_df.show(5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+------------+-------+----------+-------+-----+\n",
      "|item_type|            location|payment_type|  price|product_id|shop_id|  way|\n",
      "+---------+--------------------+------------+-------+----------+-------+-----+\n",
      "|     shoe|   {7.0814, 56.5147}|      paypal|58.6024|        12|     36|store|\n",
      "|    shirt| {20.7594, 169.8936}|        card|29.6226|        14|     15|store|\n",
      "|     shoe| {93.5029, 159.9378}|        cash|33.0585|        11|     37|store|\n",
      "|     shoe|{59.1923, -178.1721}|      paypal|97.9282|        81|     90|store|\n",
      "|     jean| {62.8714, 128.6758}|      paypal| 1.9822|        14|     61|  web|\n",
      "+---------+--------------------+------------+-------+----------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "c3b1c6ef8d45fd7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T10:31:39.617223Z",
     "start_time": "2025-04-27T10:31:39.512938Z"
    }
   },
   "source": [
    "stock_df.show(5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|product_id|quantity|\n",
      "+----------+--------+\n",
      "|         1|      34|\n",
      "|         2|      97|\n",
      "|         3|      70|\n",
      "|         4|      66|\n",
      "|         5|      75|\n",
      "+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "68aa96d447b00a03",
   "metadata": {},
   "source": [
    "## Solución de la tarea\n",
    "\n",
    "Debéis crear un programa Spark 2.x utilizando el lenguaje Python y resolver las siguientes tareas (usando tanto del DataFrame API como Spark SQL):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b3b91ce1770a32",
   "metadata": {},
   "source": [
    "### 1. Los 10 productos más comprados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4346bbfbaa4662ce",
   "metadata": {},
   "source": [
    "#### Spark DataFrame:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "## TODO : Solve this using Spark",
   "id": "b6489b134f25d05c"
  },
  {
   "cell_type": "markdown",
   "id": "a40f0e125c0d64ef",
   "metadata": {},
   "source": [
    "#### SQL:\n",
    "\n",
    "Primeramente, debemos crear una vista temporal:"
   ]
  },
  {
   "cell_type": "code",
   "id": "3480022089621bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T10:31:39.811377Z",
     "start_time": "2025-04-27T10:31:39.789168Z"
    }
   },
   "source": [
    "purchases_df.createOrReplaceTempView(\"purchases\")"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "b9e770b265da648c",
   "metadata": {},
   "source": [
    "Una vez hecho esto, ejecutamos la consulta en SQL nativo:"
   ]
  },
  {
   "cell_type": "code",
   "id": "5e22f457f3dccc55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T10:31:40.716344Z",
     "start_time": "2025-04-27T10:31:40.002217Z"
    }
   },
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT product_id, COUNT(*) as count\n",
    "    FROM purchases\n",
    "    GROUP BY product_id\n",
    "    ORDER BY count DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|product_id|count|\n",
      "+----------+-----+\n",
      "|        64|   50|\n",
      "|        81|   45|\n",
      "|         3|   44|\n",
      "|        61|   43|\n",
      "|        31|   43|\n",
      "|        36|   43|\n",
      "|        60|   43|\n",
      "|        69|   42|\n",
      "|        30|   42|\n",
      "|         7|   41|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "9fd9ebbdf85035f6",
   "metadata": {},
   "source": [
    "### 2. Porcentaje de compra de cada tipo de producto (`item_type`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73676a2094a5e56",
   "metadata": {},
   "source": [
    "#### Spark DataFrame:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "## TODO : Solve this using Spark",
   "id": "667902c3cc0a5019"
  },
  {
   "cell_type": "markdown",
   "id": "22aa11345c39e01",
   "metadata": {},
   "source": [
    "#### SQL:"
   ]
  },
  {
   "cell_type": "code",
   "id": "b4bfb9a657409d76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T10:31:41.272943Z",
     "start_time": "2025-04-27T10:31:40.755212Z"
    }
   },
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT item_type,\n",
    "           (COUNT(*) / (SELECT COUNT(*) FROM purchases)) * 100 as percentage\n",
    "    FROM purchases\n",
    "    GROUP BY item_type\n",
    "    ORDER BY percentage DESC\n",
    "\"\"\").show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|item_type|        percentage|\n",
      "+---------+------------------+\n",
      "|     shoe| 20.87978306718891|\n",
      "|     jean| 20.75926483880687|\n",
      "|    shirt|19.584212112081953|\n",
      "|  trouser| 19.55408255498644|\n",
      "|   jacket|19.222657426935825|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "4c1ac58457acc8",
   "metadata": {},
   "source": [
    "### 3. Obtener los 3 productos más comprados por cada tipo de producto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef5631ba6aec8cb",
   "metadata": {},
   "source": [
    "#### Spark DataFrame:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "## TODO : Solve this using Spark",
   "id": "62dadd84eb543d8e"
  },
  {
   "cell_type": "markdown",
   "id": "9b677ac5b39c17cc",
   "metadata": {},
   "source": [
    "#### Spark SQL Nativo:"
   ]
  },
  {
   "cell_type": "code",
   "id": "ad7b6ca853c55c63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T10:31:41.901907Z",
     "start_time": "2025-04-27T10:31:41.310921Z"
    }
   },
   "source": [
    "spark.sql(\"\"\"\n",
    "    WITH product_counts AS (\n",
    "        SELECT\n",
    "            product_id,\n",
    "            item_type,\n",
    "            COUNT(*) AS purchase_count\n",
    "        FROM\n",
    "            purchases\n",
    "        GROUP BY\n",
    "            item_type, product_id\n",
    "    ),\n",
    "    ranked_products AS (\n",
    "        SELECT\n",
    "            product_id,\n",
    "            item_type,\n",
    "            purchase_count,\n",
    "            ROW_NUMBER() OVER (PARTITION BY item_type ORDER BY purchase_count DESC) AS rank\n",
    "        FROM\n",
    "            product_counts\n",
    "    )\n",
    "    SELECT\n",
    "        product_id,\n",
    "        item_type,\n",
    "        purchase_count,\n",
    "        rank\n",
    "    FROM\n",
    "        ranked_products\n",
    "    WHERE\n",
    "        rank <= 3\n",
    "    ORDER BY\n",
    "        item_type, rank\n",
    "\"\"\").show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------+----+\n",
      "|product_id|item_type|purchase_count|rank|\n",
      "+----------+---------+--------------+----+\n",
      "|        64|   jacket|            12|   1|\n",
      "|        12|   jacket|            12|   2|\n",
      "|        92|   jacket|            11|   3|\n",
      "|        76|     jean|            14|   1|\n",
      "|        90|     jean|            12|   2|\n",
      "|        47|     jean|            11|   3|\n",
      "|         3|    shirt|            13|   1|\n",
      "|        59|    shirt|            12|   2|\n",
      "|        54|    shirt|            12|   3|\n",
      "|        85|     shoe|            16|   1|\n",
      "|        96|     shoe|            15|   2|\n",
      "|        69|     shoe|            15|   3|\n",
      "|        23|  trouser|            13|   1|\n",
      "|        25|  trouser|            12|   2|\n",
      "|        68|  trouser|            12|   3|\n",
      "+----------+---------+--------------+----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "8354d495e516709d",
   "metadata": {},
   "source": [
    "### 4. Obtener los productos que son más caros que la media del precio de los productos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e04db0d1558fb9",
   "metadata": {},
   "source": [
    "#### Spark DataFrame:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "## TODO : Solve this using Spark",
   "id": "4ca991ebde4bf4e3"
  },
  {
   "cell_type": "markdown",
   "id": "61f138c9fc5de53c",
   "metadata": {},
   "source": [
    "#### Spark SQL Nativo:"
   ]
  },
  {
   "cell_type": "code",
   "id": "ca8126be5b68f294",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T10:31:42.522511Z",
     "start_time": "2025-04-27T10:31:41.940558Z"
    }
   },
   "source": [
    "spark.sql(\"\"\"\n",
    "    WITH average_price AS (\n",
    "        SELECT AVG(price) AS avg_price FROM purchases\n",
    "    )\n",
    "    SELECT\n",
    "        DISTINCT p.product_id,\n",
    "        p.item_type,\n",
    "        p.price,\n",
    "        ROUND(a.avg_price, 2) AS average_price\n",
    "    FROM\n",
    "        purchases p\n",
    "    CROSS JOIN\n",
    "        average_price a\n",
    "    WHERE\n",
    "        p.price > a.avg_price\n",
    "    ORDER BY\n",
    "        p.price ASC\n",
    "\"\"\").show()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+-------------+\n",
      "|product_id|item_type|  price|average_price|\n",
      "+----------+---------+-------+-------------+\n",
      "|         1|     shoe|49.7916|        49.78|\n",
      "|        94|  trouser|49.7948|        49.78|\n",
      "|        10|     jean|49.8007|        49.78|\n",
      "|        86|  trouser|49.8073|        49.78|\n",
      "|        54|  trouser|49.8356|        49.78|\n",
      "|        94|  trouser|49.8365|        49.78|\n",
      "|        40|     shoe|49.8797|        49.78|\n",
      "|        51|   jacket|49.8858|        49.78|\n",
      "|        52|    shirt|49.8867|        49.78|\n",
      "|        60|  trouser|49.9317|        49.78|\n",
      "|         6|    shirt|49.9319|        49.78|\n",
      "|        92|   jacket|49.9374|        49.78|\n",
      "|        30|     shoe| 49.963|        49.78|\n",
      "|        85|   jacket|49.9758|        49.78|\n",
      "|        15|    shirt|50.0076|        49.78|\n",
      "|        71|    shirt|50.0145|        49.78|\n",
      "|         8|    shirt|50.0415|        49.78|\n",
      "|        38|  trouser|  50.06|        49.78|\n",
      "|        46|   jacket|50.0828|        49.78|\n",
      "|        90|     jean|50.1206|        49.78|\n",
      "+----------+---------+-------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "e381acac2bf589fb",
   "metadata": {},
   "source": [
    "\n",
    "### 5. Indicar la tienda que ha vendido más productos."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Spark DataFrame:",
   "id": "a3737acb9b2e6dd7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T10:31:59.277293Z",
     "start_time": "2025-04-27T10:31:59.128416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "(\n",
    "    purchases_df.groupBy(\"shop_id\")\n",
    "    .count()\n",
    "    .orderBy(F.col(\"count\").desc())\n",
    "    .limit(1)\n",
    "    .show()\n",
    ")"
   ],
   "id": "e84fe9ccf6a8bea7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|shop_id|count|\n",
      "+-------+-----+\n",
      "|     69|   47|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Spark SQL Nativo:",
   "id": "bafbecf2e9ae82b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "## TODO : Solve this using Spark",
   "id": "333ab5cf1e714ca0"
  },
  {
   "cell_type": "markdown",
   "id": "dab631f498019928",
   "metadata": {},
   "source": [
    "### 6. Indicar la tienda que ha facturado más dinero."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Spark DataFrame:",
   "id": "194c2c1e40f477c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T10:33:27.344207Z",
     "start_time": "2025-04-27T10:33:27.161281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "(\n",
    "    purchases_df.groupBy(\"shop_id\")\n",
    "    .agg(F.sum(\"price\").alias(\"revenue\"))\n",
    "    .orderBy(F.col(\"revenue\").desc())\n",
    "    .limit(1)\n",
    "    .show()\n",
    ")"
   ],
   "id": "fd8409bda92c530f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|shop_id|           revenue|\n",
      "+-------+------------------+\n",
      "|     69|2444.8898000000013|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Spark SQL Nativo:",
   "id": "5958a80ee87aa62b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "## TODO : Solve this using Spark",
   "id": "97ab6b786a5c88d8"
  },
  {
   "cell_type": "markdown",
   "id": "e907912aa69217d6",
   "metadata": {},
   "source": [
    "### 7. Dividir el mundo en 5 áreas geográficas iguales según la longitud (location.lon) y agregar una columna con el nombre del área geográfica (Area1: - 180 a - 108, Area2: - 108 a - 36, Area3: - 36 a 36, Area4: 36 a 108, Area5: 108 a 180), ..."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "## TODO : Solve this using Spark",
   "id": "619e98910ce487ba"
  },
  {
   "cell_type": "markdown",
   "id": "b9bcfb6c367184d",
   "metadata": {},
   "source": [
    "#### 7.1. ¿En qué área se utiliza más PayPal?"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Spark DataFrame:",
   "id": "eb1097da702fdde"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "## TODO : Solve this using Spark",
   "id": "d1eac6b487c48ccb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Spark SQL Nativo:",
   "id": "3223df8e57fc0036"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "## TODO : Solve this using Spark",
   "id": "f336e3b59236ea0"
  },
  {
   "cell_type": "markdown",
   "id": "7aa74a45c156c39d",
   "metadata": {},
   "source": [
    "#### 7.2. ¿Cuáles son los 3 productos más comprados en cada área?"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Spark DataFrame:",
   "id": "61fcd620aca0a4a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "## TODO : Solve this using Spark",
   "id": "68163c06b78a8b1f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Spark SQL Nativo:",
   "id": "7a3c9688c3fc6bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "## TODO : Solve this using Spark",
   "id": "1103b57b8ca8e2e7"
  },
  {
   "cell_type": "markdown",
   "id": "73f35986ad256e0d",
   "metadata": {},
   "source": [
    "#### 7.3. ¿Qué área ha facturado menos dinero?"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Spark DataFrame:",
   "id": "3c2b7543607c720"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "## TODO : Solve this using Spark",
   "id": "fb851e435d926ee5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Spark SQL Nativo:",
   "id": "6e5262bc880e50a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "## TODO : Solve this using Spark",
   "id": "f08ee6abdb430286"
  },
  {
   "cell_type": "markdown",
   "id": "1ff3c4e2833b7ef",
   "metadata": {},
   "source": [
    "### 8. Indicar los productos que no tienen stock suficiente para las compras realizadas."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Spark DataFrame:",
   "id": "189aeea593caea3a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T10:37:29.937254Z",
     "start_time": "2025-04-27T10:37:29.760041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "purchases_count_df = (\n",
    "    purchases_df\n",
    "    .groupBy(\"product_id\")\n",
    "    .count()\n",
    "    .withColumnRenamed(\"count\", \"purchases_made\")\n",
    ")\n",
    "\n",
    "insufficient_stock_df = (\n",
    "    stock_df\n",
    "    .join(purchases_count_df, \"product_id\")\n",
    "    .filter(F.col(\"quantity\") < F.col(\"purchases_made\"))\n",
    "    .withColumnRenamed(\"quantity\", \"quantity_in_stock\")\n",
    ")\n",
    "\n",
    "insufficient_stock_df.show()"
   ],
   "id": "2e6e2d978ff14841",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+--------------+\n",
      "|product_id|quantity_in_stock|purchases_made|\n",
      "+----------+-----------------+--------------+\n",
      "|        29|               25|            38|\n",
      "|         1|               34|            39|\n",
      "|        37|               22|            38|\n",
      "+----------+-----------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Spark SQL Nativo:",
   "id": "59b0baa6475fd380"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "## TODO : Solve this using Spark",
   "id": "6c3afea3055a89a7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
